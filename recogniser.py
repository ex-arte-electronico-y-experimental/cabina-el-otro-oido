# -*- coding: utf-8 -*-
"""colab_examples.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb

# Live Colab Example

## Dependencies and Imports
"""

import os
from os.path import exists
from collections import deque
import webrtcvad

# silero imports
import torch
import random
from glob import glob
import sounddevice as sd
import numpy as np
from omegaconf import OmegaConf
from utils import (init_jit_model, read_audio, read_batch, prepare_model_input, init_jit_model_local)

# misc
from halo import Halo

MODEL_PATH = "./models/es_v1_jit.model"
DEVICE = torch.device('cpu')   # you can use any pytorch device
# MODELS = OmegaConf.load('models.yml')

SPINNER = Halo(spinner='dots')

# STT class for scoping:

class SpeechRecognition:
	def __init__(self):
		print("Cargando modelo...")
		# eventually download model from MODELS.stt_models.es.latest.jit so there's no fetching
		self.model, self.decoder = init_jit_model_local(MODEL_PATH, device=DEVICE)
		print("Modelo listo!")

		self.sample_rate = 16000
		sd.default.samplerate = 16000
		self.frame_size = 480 # approx. 30m, webrtcvad imposes limits of 10, 20, 30 ms frames
		self.frame_duration_ms = 1000 * self.frame_size // self.sample_rate
		
		self.vad = webrtcvad.Vad(3)
		self.vad_padding_ms = 300
		self.vad_ring_buffer = deque(maxlen = self.vad_padding_ms // self.frame_duration_ms)

		self.float_frame = 0 # will change on every audio clock cycle, to be accesed by vad_switch
		self.bytes_frame = None # will change on every audio clock cycle, to be accesed by vad_switch
		self.int16info = np.iinfo(np.int16)
		self.int16tofloat = lambda x: x/abs(self.int16info.min) if x < 0 else x/self.int16info.max
		self.blockaccum = []
		self.utterance_transcription = ""

		self.vad_triggered = False

	# VAD inference switch:
	def vad_switch(self, ratio=0.75):
		"""Generator that yields series of consecutive audio frames comprising each utterence, separated by yielding a single None.
			Determines voice activity by ratio of frames in padding_ms. Uses a buffer to include padding_ms prior to being triggered.
			Example: (frame, ..., frame, None, frame, ..., frame, None, ...)
						|---utterence---|        |---utterence---|
		"""

		is_speech = self.vad.is_speech(self.bytes_frame, self.sample_rate)

		if not self.vad_triggered:
			self.vad_ring_buffer.append((self.float_frame, is_speech))
			num_voiced = len([f for f, speech in self.vad_ring_buffer if speech])
			# print(f"vad_generator: not triggered, num_voiced: {num_voiced}")
			if num_voiced > ratio * self.vad_ring_buffer.maxlen:
				self.vad_triggered = True
				for f, s in self.vad_ring_buffer:
					yield f
				self.vad_ring_buffer.clear()

		else:
			yield self.float_frame
			self.vad_ring_buffer.append((self.float_frame, is_speech))
			num_unvoiced = len([f for f, speech in self.vad_ring_buffer if not speech])
			if num_unvoiced > ratio * self.vad_ring_buffer.maxlen:
				self.vad_triggered = False
				yield None
				self.vad_ring_buffer.clear()

	# wav to text method
	def audio_to_text(self, batch):
		# batch: [torch.Tensor], one per audio stream chunk
		minput = prepare_model_input(batch, device=DEVICE)
		# print(f"JORGE - torch input contains: {minput}:\n")
		output = self.model(minput)
		return self.decoder(output[0].cpu())


	def audio_callback(self, indata, frames, time, status):
		# indata = (32767*indata).astype(np.int16)
		# self.float_frame = indata
		# self.bytes_frame = self.float_frame.astype(np.float16).tobytes()
		self.bytes_frame = indata
		self.float_frame = np.fromiter(map(self.int16tofloat, np.frombuffer(indata, dtype=np.int16)), float)

		# print(f"debug, vad_switch_generator type: {type(self.vad_switch_generator)}")
		for frame in self.vad_switch():
			if frame is not None:
				SPINNER.start(text="Esperando a que termine la frase...")

				frame = frame.reshape((1,frame.shape[0])) # convert column vector into row vector
				self.blockaccum = np.append(self.blockaccum, frame)
			else:
				SPINNER.stop()
				SPINNER.start(text="Transcribiendo voz...")

				batch = [torch.tensor(self.blockaccum).squeeze(0)]
				self.blockaccum = []
				interim_transcript = self.audio_to_text(batch)
				# print(interim_transcript)
				if interim_transcript != self.utterance_transcription:
					self.utterance_transcription = interim_transcript
					print(f"\n Ha dicho: {self.utterance_transcription}")

				SPINNER.stop()
	
	def start(self):
		with sd.RawInputStream(channels=1, callback=self.audio_callback, samplerate=self.sample_rate, blocksize=self.frame_size, dtype='int16', device=None):
			print("Escuchando...")
			while True:
				sd.sleep(1)


if __name__ == "__main__":
	print(f"Available devices: {sd.query_devices(kind='input')} \n Default: {sd.default.device}")
	# create recogniser object
	stt = SpeechRecognition()
	stt.start()